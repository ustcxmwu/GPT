{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 简单自注意力"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f488708437cb4a43"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "attn_weights = F.softmax(raw_weights, dim=2)\n",
    "attn_outputs = torch.bmm(attn_weights, x)\n",
    "print(attn_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T09:35:47.052931Z",
     "start_time": "2024-06-11T09:35:46.258967Z"
    }
   },
   "id": "db5976e82194dfb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2478, -0.5643,  0.6200,  0.2377],\n",
      "         [ 0.2983, -0.8605,  0.8004,  0.3084],\n",
      "         [ 0.3253,  0.2624,  0.0781,  0.0707]],\n",
      "\n",
      "        [[-0.1451,  0.4304,  0.4068, -1.3426],\n",
      "         [-0.6821, -0.1725, -1.1295,  0.8898],\n",
      "         [-1.4510, -0.3237,  0.5355, -0.1539]]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 标准自注意力"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a7de25afa54ff36"
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "linear_q = torch.nn.Linear(4, 4)\n",
    "linear_k = torch.nn.Linear(4, 4)\n",
    "linear_v = torch.nn.Linear(4, 4)\n",
    "\n",
    "Q = linear_q(x)\n",
    "K = linear_k(x)\n",
    "V = linear_v(x)\n",
    "\n",
    "raw_weights = torch.bmm(Q, K.transpose(1, 2))\n",
    "print(raw_weights)\n",
    "\n",
    "scale_factor = K.size(-1)**0.5\n",
    "scale_weights = raw_weights / scale_factor\n",
    "print(scale_weights)\n",
    "\n",
    "attn_weights = F.softmax(scale_weights, dim=2)\n",
    "attn_outputs = torch.bmm(attn_weights, V)\n",
    "print(f\"加权自注意力: {attn_outputs}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T09:35:48.677722Z",
     "start_time": "2024-06-11T09:35:48.670756Z"
    }
   },
   "id": "e43bbae6e8d06333",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2342,  0.1263, -0.3857],\n",
      "         [ 0.5356,  0.1055,  0.8644],\n",
      "         [ 0.7031,  0.8428,  0.7917]],\n",
      "\n",
      "        [[-0.6063,  0.7535,  0.4628],\n",
      "         [-0.0733,  1.0862,  0.8365],\n",
      "         [ 0.2903, -0.4481, -0.4775]]], grad_fn=<BmmBackward0>)\n",
      "tensor([[[-0.1171,  0.0631, -0.1929],\n",
      "         [ 0.2678,  0.0527,  0.4322],\n",
      "         [ 0.3515,  0.4214,  0.3958]],\n",
      "\n",
      "        [[-0.3032,  0.3767,  0.2314],\n",
      "         [-0.0366,  0.5431,  0.4183],\n",
      "         [ 0.1451, -0.2240, -0.2387]]], grad_fn=<DivBackward0>)\n",
      "加权自注意力: tensor([[[ 0.3119,  0.4292,  0.0073,  0.5235],\n",
      "         [ 0.5184,  0.4523, -0.0223,  0.5037],\n",
      "         [ 0.3849,  0.4396, -0.0026,  0.5178]],\n",
      "\n",
      "        [[ 0.0177,  0.0955, -0.0859,  0.6672],\n",
      "         [ 0.0196,  0.0938, -0.0775,  0.6586],\n",
      "         [ 0.0174,  0.0821,  0.0317,  0.5703]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 多头自注意力"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e50a15d8817e2a1"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "\n",
    "num_heads = 2\n",
    "head_dim = 2\n",
    "\n",
    "assert x.size(-1) == num_heads * head_dim\n",
    "\n",
    "linear_q = torch.nn.Linear(4, 4)\n",
    "linear_k = torch.nn.Linear(4, 4)\n",
    "linear_v = torch.nn.Linear(4, 4)\n",
    "\n",
    "Q = linear_q(x)\n",
    "K = linear_k(x)\n",
    "V = linear_v(x)\n",
    "\n",
    "def split_heads(tensor, num_heads):\n",
    "    batch_size, seq_len, feature_dim = tensor.size()\n",
    "    head_dim = feature_dim // num_heads\n",
    "    output = tensor.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "    return output\n",
    "\n",
    "Q = split_heads(Q, num_heads)\n",
    "K = split_heads(K, num_heads)\n",
    "V = split_heads(V, num_heads)\n",
    "\n",
    "raw_weights = torch.matmul(Q, K.transpose(-2, -1))\n",
    "print(raw_weights)\n",
    "\n",
    "scale_factor = K.size(-1)**0.5\n",
    "scale_weights = raw_weights / scale_factor\n",
    "print(scale_weights)\n",
    "\n",
    "attn_weights = F.softmax(scale_weights, dim=-1)\n",
    "attn_outputs = torch.matmul(attn_weights, V)\n",
    "\n",
    "def combine_heads(tensor):\n",
    "    batch_size, num_heads, seq_len, head_dim = tensor.size()\n",
    "    feature_dim = num_heads * head_dim\n",
    "    output = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, feature_dim)\n",
    "    return output\n",
    "\n",
    "attn_outputs = combine_heads(attn_outputs)\n",
    "\n",
    "linear_out = torch.nn.Linear(4, 4)\n",
    "attn_outputs = linear_out(attn_outputs)\n",
    "\n",
    "print(f\"多头自注意力: {attn_outputs}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T09:35:50.390720Z",
     "start_time": "2024-06-11T09:35:50.372158Z"
    }
   },
   "id": "6102c57502d36b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0499, -0.1048, -0.1749],\n",
      "          [ 0.1599, -0.2591, -0.4439],\n",
      "          [ 0.0460, -0.0812, -0.1379]],\n",
      "\n",
      "         [[-0.6317, -0.1134,  0.0739],\n",
      "          [ 0.5791,  0.2177,  0.0511],\n",
      "          [ 0.2978, -0.0355, -0.1278]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5702,  0.0161, -0.1847],\n",
      "          [-0.6763, -0.0492,  0.2023],\n",
      "          [-0.6830, -0.1313,  0.1590]],\n",
      "\n",
      "         [[-1.3637,  0.3557, -0.7735],\n",
      "          [ 0.6436, -0.3130,  0.3675],\n",
      "          [-1.4432,  0.9831, -0.8288]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[[ 0.0353, -0.0741, -0.1236],\n",
      "          [ 0.1130, -0.1832, -0.3139],\n",
      "          [ 0.0325, -0.0574, -0.0975]],\n",
      "\n",
      "         [[-0.4467, -0.0802,  0.0522],\n",
      "          [ 0.4095,  0.1539,  0.0361],\n",
      "          [ 0.2106, -0.0251, -0.0904]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4032,  0.0114, -0.1306],\n",
      "          [-0.4782, -0.0348,  0.1431],\n",
      "          [-0.4830, -0.0929,  0.1124]],\n",
      "\n",
      "         [[-0.9643,  0.2515, -0.5470],\n",
      "          [ 0.4551, -0.2213,  0.2599],\n",
      "          [-1.0205,  0.6951, -0.5861]]]], grad_fn=<DivBackward0>)\n",
      "多头自注意力: tensor([[[ 0.3728, -0.3365, -0.1035,  0.2045],\n",
      "         [ 0.4210, -0.4050,  0.0230,  0.1513],\n",
      "         [ 0.3992, -0.3933, -0.0116,  0.1470]],\n",
      "\n",
      "        [[ 0.3791, -0.3055, -0.0553, -0.0027],\n",
      "         [ 0.4443, -0.2505, -0.1323,  0.2308],\n",
      "         [ 0.4317, -0.4126,  0.0434,  0.0859]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T09:35:51.436266Z",
     "start_time": "2024-06-11T09:35:51.434130Z"
    }
   },
   "id": "d33f10ce14a22846",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "72bb3d7e84227b54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
